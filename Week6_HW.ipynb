{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e50351-15c7-4379-9369-cd41cd7ac272",
   "metadata": {},
   "source": [
    "# (Homework) Week 6 - DataScience Bootcamp Fall 2025\n",
    "\n",
    "All solution cells are replaced with `# TODO` placeholders so you can fill them in.\n",
    "\n",
    "**Name:** \\\n",
    "**Email:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ae2a1-9b4d-4b8e-87a8-fd32d8c107c8",
   "metadata": {},
   "source": [
    "### Problem 1: Dataset Splitting\n",
    "\n",
    "1. You have recordings of 44 phones from 100 people; each person records ~200 phones/day for 5 days.\n",
    "   - Design a valid training/validation/test split strategy that ensures the model generalizes to **new speakers**.\n",
    "\n",
    "2. You now receive an additional dataset of 10,000 phone recordings from **Kilian**, a single speaker.\n",
    "   - You must train a model that performs well **specifically for Kilian**, while also maintaining generalization.\n",
    "\n",
    "*Describe your proposed split strategy and reasoning.* (Theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca1215f-525a-4fd4-8653-842279e505da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo\n",
    "A) Baseline corpus (100 speakers × ~200 phones/day × 5 days) — speaker-disjoint split\n",
    "Unit of stratification: speaker (not utterance/day) to avoid leakage.\n",
    "Split: 70% train, 15% validation, 15% test by speaker (e.g., 70/15/15 speakers).\n",
    "Per-speaker temporal hygiene: within each split, keep all 5 days for a speaker together (no mixing a person’s days across splits).\n",
    "Label balance: stratify so the 44 phone classes have similar histograms across splits.\n",
    "Rationale: ensures the model never sees test speakers during training and evaluates true speaker generalization.\n",
    "B) Adding the single-speaker Kilian set (10,000 phones)\n",
    "Two objectives: (1) strong general model and (2) Kilian-specific performance. Use a two-stage pipeline:\n",
    "Train a base model on the 100-speaker corpus only (Kilian excluded).\n",
    "Tune on the 15% speaker-disjoint validation split.\n",
    "Keep the 15% test split frozen for final cross-speaker generalization reporting.\n",
    "Specialize for Kilian via fine-tuning or adapters.\n",
    "Split Kilian by time: e.g., Days 1–3 for fine-tune, Day 4 for early stopping/selection, Day 5 held out as Kilian-test.\n",
    "Technique:\n",
    "• Option A: fine-tune last layers (or add lightweight adapters) on Kilian-train; early-stop on Kilian-val.\n",
    "• Option B: train a small speaker-adaptation module (e.g., affine feature normalization, i-vectors/x-vectors, prompt embeddings) while freezing most base weights.\n",
    "Regularization: early stopping, small learning rate, weight decay; optionally rehearsal (sample from multi-speaker train) to prevent catastrophic forgetting.\n",
    "Reporting: give two test numbers—(i) multi-speaker test (speaker-disjoint) and (ii) Kilian-test (held-out day).\n",
    "C) Practical notes\n",
    "No leakage: Kilian data must not appear in the multi-speaker validation/test sets.\n",
    "Imbalance: if Kilian’s phone distribution differs, use importance weighting or class-balanced losses during specialization.\n",
    "Robustness: optionally build a small “few-shot speaker set” (5–10 other single-speaker chunks) to validate that the adaptation recipe generalizes beyond Kilian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b7930-1fef-4fd2-ac71-1467e8b165e8",
   "metadata": {},
   "source": [
    "### Problem 2: K-Nearest Neighbors\n",
    "\n",
    "1. **1-NN Classification:** Given dataset:\n",
    "\n",
    "   Positive: (1,2), (1,4), (5,4)\n",
    "\n",
    "   Negative: (3,1), (3,2)\n",
    "\n",
    "   Plot the 1-NN decision boundary and classify new points visually.\n",
    "\n",
    "2. **Feature Scaling:** Consider dataset:\n",
    "\n",
    "   Positive: (100,2), (100,4), (500,4)\n",
    "\n",
    "   Negative: (300,1), (300,2)\n",
    "\n",
    "   What would the 1-NN classify point (500,1) as **before and after scaling** to [0,1] per feature?\n",
    "\n",
    "3. **Handling Missing Values:** How can you modify K-NN to handle missing features in a test point?\n",
    "\n",
    "4. **High-dimensional Data:** Why can K-NN still work well for images even with thousands of pixels?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f66d2-4e36-4e30-8ef5-72d9b7986ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pos = np.array([(1,2), (1,4), (5,4)])\n",
    "neg = np.array([(3,1), (3,2)])\n",
    "\n",
    "def nn_predict(Xtrain, ytrain, x):\n",
    "    d = np.sum((Xtrain - x)**2, axis=1)\n",
    "    return ytrain[np.argmin(d)]\n",
    "\n",
    "\n",
    "Xtrain = np.vstack([pos, neg])\n",
    "ytrain = np.hstack([np.ones(len(pos)), -np.ones(len(neg))])\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(0,6,300), np.linspace(0,6,300))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "pred = np.array([nn_predict(Xtrain, ytrain, p) for p in grid]).reshape(xx.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, pred, alpha=0.15)\n",
    "plt.scatter(pos[:,0], pos[:,1], marker='o', label='Positive (+1)')\n",
    "plt.scatter(neg[:,0], neg[:,1], marker='x', label='Negative (-1)')\n",
    "plt.legend()\n",
    "plt.title(\"1-NN decision regions (unscaled)\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pos2 = np.array([(100,2), (100,4), (500,4)])\n",
    "neg2 = np.array([(300,1), (300,2)])\n",
    "xq = np.array([500,1])\n",
    "\n",
    "\n",
    "def euclid(a,b): return np.sqrt(np.sum((a-b)**2))\n",
    "dists_before = [(tuple(p), euclid(xq, p), +1) for p in pos2] + [(tuple(n), euclid(xq, n), -1) for n in neg2]\n",
    "nearest_before = min(dists_before, key=lambda t: t[1])\n",
    "print(\"Nearest before scaling:\", nearest_before, \"→ class =\", nearest_before[2])\n",
    "\n",
    "\n",
    "all_ = np.vstack([pos2, neg2, xq])\n",
    "mins = all_.min(axis=0); maxs = all_.max(axis=0)\n",
    "scale = lambda z: (z - mins)/(maxs - mins + 1e-12)\n",
    "\n",
    "pos2s = np.array([scale(p) for p in pos2])\n",
    "neg2s = np.array([scale(n) for n in neg2])\n",
    "xqs = scale(xq)\n",
    "\n",
    "dists_after = [(tuple(p), euclid(xqs, p), +1) for p in pos2s] + [(tuple(n), euclid(xqs, n), -1) for n in neg2s]\n",
    "nearest_after = min(dists_after, key=lambda t: t[1])\n",
    "print(\"Nearest after scaling:\", nearest_after, \"→ class =\", nearest_after[2])\n",
    "\n",
    "print(\"\\\\nInterpretation: (500,1) is +1 before scaling (closest to (500,4)), but -1 after scaling (closest to (300,1)).\")\n",
    "\n",
    "\n",
    "print(\\\"\\\"\\\"\\\\nKNN with missing features:\n",
    "- Compute distances over observed dimensions only (ignore missing dims) and divide by sqrt(#observed) to keep scale.\n",
    "- Or impute first (mean/median/knn-impute) then do standard KNN.\n",
    "- Use distance-weighted voting to reduce ties and sensitivity.\n",
    "\\\"\\\"\\\")\n",
    "\n",
    "\n",
    "print(\\\"\\\"\\\"Why KNN can still work for images:\n",
    "- If using good features (e.g., embeddings from a CNN) the manifold is low-dimensional → neighbors are meaningful.\n",
    "- Locality + large datasets → nearest neighbors often share labels.\n",
    "- Use approximate NN, cosine distance, and dimensionality reduction (PCA) to mitigate the curse of dimensionality.\n",
    "\\\"\\\"\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f766e-e313-4c28-a2af-b8a7985e3db7",
   "metadata": {},
   "source": [
    "### Problem 3: Part 1\n",
    "\n",
    "You are given a fully trained Perceptron model with weight vector **w**, along with training set **D_TR** and test set **D_TE**.\n",
    "\n",
    "1. Your co-worker suggests evaluating $h(x) = sign(w \\cdot x)$ for every $(x, y)$ in D_TR and D_TE. Does this help determine whether test error is higher than training error?\n",
    "2. Why is there no need to compute training error explicitly for the Perceptron algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca95dc-c37e-4f56-ab0a-9913bde3079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo\n",
    "- Evaluating on both D_TR and D_TE is fine for reporting, but model selection should use a validation set to avoid test leakage.\n",
    "- For a classic Perceptron on linearly separable data, once it converges it makes zero training error (it only updates on mistakes).\n",
    "  Hence you typically don't need to compute training error explicitly.\n",
    "- Test error can be higher due to distribution shift and finite sample effects; margin-based generalization bounds depend on the margin and w.\n",
    "- Good practice: (train on train) → (tune on validation) → (report once on test)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e8682-2b9f-4b15-a38e-2d3ec75591dc",
   "metadata": {},
   "source": [
    "### Problem 3: Two-point 2D Dataset (Part 2)\n",
    "\n",
    "Run the Perceptron algorithm **by hand or in code** on the following data:\n",
    "\n",
    "1. Positive class: (10, -2)\n",
    "2. Negative class: (12, 2)\n",
    "\n",
    "Start with $w_0 = (0, 0)$ and a learning rate of 1.\n",
    "\n",
    "- Compute how many updates are required until convergence.\n",
    "- Write down the sequence of $w_i$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4597a-387e-4d5d-bbe3-f621afd13625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo\n",
    "\n",
    "\n",
    "**Given:**\n",
    "- Positive class: (10, -2) → y = +1  \n",
    "- Negative class: (12, 2) → y = -1  \n",
    "- Learning rate η = 1  \n",
    "- Start w₀ = (0, 0)  \n",
    "- Update rule: w ← w + yx (no bias term)\n",
    "\n",
    "---\n",
    "\n",
    "**Step-by-step updates:**\n",
    "\n",
    "t = 1 (x⁺ = (10, -2), y = +1):  \n",
    "w₀ = (0, 0) → y(w·x) = 0 → update  \n",
    "w₁ = (0, 0) + (+1)*(10, -2) = (10, -2)\n",
    "\n",
    "t = 2 (x⁻ = (12, 2), y = -1):  \n",
    "w₁·x = 10*12 + (-2)*2 = 120 - 4 = 116  \n",
    "y(w·x) = (-1)*116 = -116 ≤ 0 → update  \n",
    "w₂ = (10, -2) + (-1)*(12, 2) = (-2, -4)\n",
    "\n",
    "t = 3 (x⁺):  \n",
    "w₂·x = (-2)*10 + (-4)*(-2) = -20 + 8 = -12 ≤ 0 → update  \n",
    "w₃ = (-2, -4) + (10, -2) = (8, -6)\n",
    "\n",
    "t = 4 (x⁻):  \n",
    "w₃·x = 8*12 + (-6)*2 = 96 - 12 = 84  \n",
    "y(w·x) = (-1)*84 = -84 ≤ 0 → update  \n",
    "w₄ = (8, -6) + (-1)*(12, 2) = (-4, -8)\n",
    "\n",
    "t = 5 (x⁺):  \n",
    "w₄·x = (-4)*10 + (-8)*(-2) = -40 + 16 = -24 ≤ 0 → update  \n",
    "w₅ = (-4, -8) + (10, -2) = (6, -10)\n",
    "\n",
    "t = 6 (x⁻):  \n",
    "w₅·x = 6*12 + (-10)*2 = 72 - 20 = 52  \n",
    "y(w·x) = (-1)*52 = -52 ≤ 0 → update  \n",
    "w₆ = (6, -10) + (-1)*(12, 2) = (-6, -12)\n",
    "\n",
    "\n",
    "\n",
    "**Pattern (closed form):**\n",
    "- Even indices:  w₂k = (-2k, -4k)\n",
    "- Odd indices:   w₂k+1 = (10 - 2k, -2 - 4k)\n",
    "\n",
    "\n",
    "\n",
    "**Convergence:**\n",
    "The algorithm does **not converge** — it cycles indefinitely between mistakes on the two samples.\n",
    "The number of updates required until convergence is **infinite**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Updates until convergence: ∞ (no convergence)  \n",
    "- Sequence of wᵢ:  \n",
    "  w₀ = (0, 0)  \n",
    "  w₁ = (10, -2)  \n",
    "  w₂ = (-2, -4)  \n",
    "  w₃ = (8, -6)  \n",
    "  w₄ = (-4, -8)  \n",
    "  w₅ = (6, -10)  \n",
    "  w₆ = (-6, -12)  \n",
    "  ... and so on.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea503e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ba29c20-59b0-456f-994e-05897175596e",
   "metadata": {},
   "source": [
    "### Problem 4: Reconstructing the Weight Vector\n",
    "\n",
    "Given the log of Perceptron updates:\n",
    "\n",
    "| x | y | count |\n",
    "|---|---|--------|\n",
    "| (0, 0, 0, 0, 4) | +1 | 2 |\n",
    "| (0, 0, 6, 5, 0) | +1 | 1 |\n",
    "| (3, 0, 0, 0, 0) | -1 | 1 |\n",
    "| (0, 9, 3, 6, 0) | -1 | 1 |\n",
    "| (0, 1, 0, 2, 5) | -1 | 1 |\n",
    "\n",
    "Assume learning rate = 1 and initial weight $w_0 = (0, 0, 0, 0, 0)$.\n",
    "\n",
    "Compute the final weight vector after all updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb261e-d6ba-4ecd-a4f4-e9b6f5104079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1., 1.],   # y=+1\n",
    "              [-1., -1.]])# y=-1\n",
    "y = np.array([+1, -1])\n",
    "\n",
    "w = np.zeros(2)\n",
    "b = 0.0\n",
    "lr = 1.0\n",
    "\n",
    "history = [(w.copy(), b, None)]\n",
    "for epoch in range(3):\n",
    "    changed = False\n",
    "    for i, (xi, yi) in enumerate(zip(X, y)):\n",
    "        if yi * (np.dot(w, xi) + b) <= 0:\n",
    "            w += lr * yi * xi\n",
    "            b += lr * yi * 0.0\n",
    "            history.append((w.copy(), b, i))\n",
    "            changed = True\n",
    "    if not changed:\n",
    "        break\n",
    "\n",
    "print(\"Sequence of weight vectors w_i:\")\n",
    "for step, (wi, bi, idx) in enumerate(history):\n",
    "    print(f\"step {step}: w={wi}, b={bi}, update_from={idx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f23b69-9f59-46c6-8103-5783fadeb7c0",
   "metadata": {},
   "source": [
    "### Problem 5: Visualizing Perceptron Convergence\n",
    "\n",
    "Implement a Perceptron on a small 2D dataset with positive and negative examples.\n",
    "\n",
    "- Plot the data points.\n",
    "- After each update, visualize the decision boundary.\n",
    "- Show how it converges to a stable separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879a3a9-de75-40a0-a901-bd2009d2b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo\n",
    "import numpy as np\n",
    "\n",
    "updates = [\n",
    "    ((0, 0, 0, 0, 4), +1, 2),\n",
    "    ((0, 0, 6, 5, 0), +1, 1),\n",
    "    ((3, 0, 0, 0, 0), -1, 1),\n",
    "    ((0, 9, 3, 6, 0), -1, 1),\n",
    "    ((0, 1, 0, 2, 5), -1, 1),\n",
    "]\n",
    "\n",
    "w = np.zeros(5, dtype=float)\n",
    "for x, y, c in updates:\n",
    "    w += y * c * np.array(x, dtype=float)\n",
    "\n",
    "print(\"Final weight vector w* =\", w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
